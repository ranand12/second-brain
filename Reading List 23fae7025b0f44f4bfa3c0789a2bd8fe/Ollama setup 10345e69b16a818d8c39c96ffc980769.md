# Ollama setup

Column: https://www.linkedin.com/feed/update/urn:li:activity:7241338694513627136
Processed: Yes
created on: September 16, 2024 7:30 AM

ğ—¥ğ˜‚ğ—»ğ—»ğ—¶ğ—»ğ—´ ğ—Ÿğ—Ÿğ— ğ˜€ ğ—¹ğ—¼ğ—°ğ—®ğ—¹ğ—¹ğ˜† - ğ—® ğ˜€ğ—¶ğ—ºğ—½ğ—¹ğ—², ğŸ¯-ğ˜€ğ˜ğ—²ğ—½ ğ˜ğ˜‚ğ˜ğ—¼ğ—¿ğ—¶ğ—®ğ—¹ ğŸï¸ I have found myself quite a few times in situations where I quickly wanted to experiment with a new LLM that was just released. And while it is easier than ever to spin up a GPU instance with one of the many cloud/service providers to get an LLM up and running, I always feel a special (and maybe irrational) joy when I get to run it locally. I have an M1 Macbook Pro with 64GB RAM and I'm able to run LLMs up to 70B (quantised) at decent speed on this machine. My tool of choice to do that is [Ollama](https://www.linkedin.com/company/ollama?trk=public_post-text), I find it dead-easy to set it up and get started, see slides below. There is also a bonus section on how to set up a ChatGPT-like web UI via [Open WebUI](https://www.linkedin.com/company/open-webui?trk=public_post-text). There are many ways to install this WebUI, check out their documentation and pick the one that is right for you.

[31](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fheikohotz_ollama-setup-activity-7241338694513627136-u8CY&trk=public_post_social-actions-reactions)   [8 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fheikohotz_ollama-setup-activity-7241338694513627136-u8CY&trk=public_post_social-actions-comments)

[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fheikohotz_ollama-setup-activity-7241338694513627136-u8CY&trk=public_post_like-cta)   [Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fheikohotz_ollama-setup-activity-7241338694513627136-u8CY&trk=public_post_comment-cta)

[Refat Ametov](https://nl.linkedin.com/in/refat-ametov?trk=public_post_comment_actor-name)

Driving Business Automation & AI Integration | Co-founder of Devstark and SpreadSimple | Stoic Mindset

8m

Running LLMs locally is satisfying, especially with Ollama making it so easy. If you're setting up, focus on optimizing your resources and consider using a WebUI for a smoother experience.

[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fheikohotz_ollama-setup-activity-7241338694513627136-u8CY&trk=public_post_comment_like)   [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fheikohotz_ollama-setup-activity-7241338694513627136-u8CY&trk=public_post_comment_reply)

[Hasan Z.](https://tr.linkedin.com/in/hasan-zamzam?trk=public_post_comment_actor-name)

Cybersecurity || Web3 || Jailbreak || AI || Human Rights

1h

Ollama is a good choice to run LLMs on your local computer as you can download and setup the UI in one place, but I noticed it's running slowly, I tried both Phi3 and LLama3 both run slowly on my CPU i5 CPU. Maybe I should give it a try on my Mac too ğŸ’¡ .

[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fheikohotz_ollama-setup-activity-7241338694513627136-u8CY&trk=public_post_comment_like)   [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fheikohotz_ollama-setup-activity-7241338694513627136-u8CY&trk=public_post_comment_reply)

[James Huckle](https://uk.linkedin.com/in/james-huckle?trk=public_post_comment_actor-name)

Chief of AI Research & Development at AutogenAI | Ex-Olympian

1h

That's my go-to setup too ğŸš€

[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fheikohotz_ollama-setup-activity-7241338694513627136-u8CY&trk=public_post_comment_like)   [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fheikohotz_ollama-setup-activity-7241338694513627136-u8CY&trk=public_post_comment_reply)

[Ben Torben-Nielsen, PhD, MBA](https://ch.linkedin.com/in/ben-torben-nielsen?trk=public_post_comment_actor-name)

AI and Innovation leader | PhD in AI | IMD EMBA | Connecting people, tech and ideas to make AI work for you

1h

This is a great resource! I have a similar setup at home [Heiko Hotz](https://uk.linkedin.com/in/heikohotz/en?trk=public_post_comment-text) (ollama, open webui and some other tools). But please don't use Gemma 2B. Gemma always responds to me like it is a cheeky, presumptious teenager :-) Other small models like Llama 3.1 7B and phi3 3.8B respond in a more helpful way.

[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fheikohotz_ollama-setup-activity-7241338694513627136-u8CY&trk=public_post_comment_like)   [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fheikohotz_ollama-setup-activity-7241338694513627136-u8CY&trk=public_post_comment_reply)

[Suraj Sharma](https://in.linkedin.com/in/creativesuraj?trk=public_post_comment_actor-name)

Founder and Principal Consultant

2h

Thatâ€™s awesome! Iâ€™ve tried Ollama too, and itâ€™s super easy to get started with. Running LLMs locally, especially on an M1 Mac, gives that extra bit of satisfaction. Great tip on the WebUI setup as wellâ€”thanks for sharing!

[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fheikohotz_ollama-setup-activity-7241338694513627136-u8CY&trk=public_post_comment_like)   [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fheikohotz_ollama-setup-activity-7241338694513627136-u8CY&trk=public_post_comment_reply)

[Christiaan Hees](https://nl.linkedin.com/in/christiaanhees?trk=public_post_comment_actor-name)

Customer Engineer at Google

36m

Oh hey, Open WebUI looks interesting. Thanks for the tip ğŸ˜€

[Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fheikohotz_ollama-setup-activity-7241338694513627136-u8CY&trk=public_post_comment_like)   [Reply](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fheikohotz_ollama-setup-activity-7241338694513627136-u8CY&trk=public_post_comment_reply)

[See more comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fheikohotz_ollama-setup-activity-7241338694513627136-u8CY&trk=public_post_see-more-comments)

To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fposts%2Fheikohotz_ollama-setup-activity-7241338694513627136-u8CY&trk=public_post_feed-cta-banner-cta)

## More Relevant Posts

- [Heiko Hotz](https://uk.linkedin.com/in/heikohotz/en?trk=public_post_main-feed-card_feed-actor-name)   1d    [86](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_what-a-week-huh-below-three-hand-picked-activity-7240970122423603200-YT4f&trk=public_post_main-feed-card_social-actions-reactions)   [5 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_what-a-week-huh-below-three-hand-picked-activity-7240970122423603200-YT4f&trk=public_post_main-feed-card_social-actions-comments)   [Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_what-a-week-huh-below-three-hand-picked-activity-7240970122423603200-YT4f&trk=public_post_main-feed-card_like-cta)   [Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_what-a-week-huh-below-three-hand-picked-activity-7240970122423603200-YT4f&trk=public_post_main-feed-card_comment-cta)
    
    Generative AI Global Blackbelt @ Google â—† Founder of NLP London
    
    What a week, huh? Below three hand-picked resources that I found useful this week to catch up on hot topics in AI. They will take you only a few minutes to read through, but hopefully will leave you thinking for a while ğŸ˜‰ ğŸ‘‰ ğ™ƒğ™–ğ™£ğ™™ğ™¨-ğ™¤ğ™£ ğ™œğ™ªğ™ğ™™ğ™š ğ™¤ğ™£ ğ™ğ™šğ™›ğ™¡ğ™šğ™˜ğ™©ğ™ğ™¤ğ™£ ğ™ğ™ªğ™£ğ™ğ™£ğ™œ by [Sebastian Raschka, PhD](https://www.linkedin.com/in/sebastianraschka?trk=public_post_main-feed-card-text): Sebastian points out that improving (instead of just growing) datasets to train LLMs has become an important theme lately. In his notebook he shows how to do that using GPT-4. ğŸ”— [https://lnkd.in/e_TDEmp5](https://lnkd.in/e_TDEmp5?trk=public_post_main-feed-card-text) ğŸ‘‰ ğ™ğ™ğ™§ğ™¨ğ™© ğ™ğ™¢ğ™¥ğ™§ğ™šğ™¨ğ™¨ğ™ğ™¤ğ™£ğ™¨ ğ™–ğ™£ğ™™ ğ™©ğ™ğ™¤ğ™ªğ™œğ™ğ™©ğ™¨ ğ™¤ğ™£ ğ™¤1 by [Jim Fan](https://www.linkedin.com/in/drjimfan?trk=public_post_main-feed-card-text): There are obviously many takes and thoughts on o1 out so far. I personally really like this one by Jim, it's short and sweet, but made me think a lot about the upcoming paradigm shift. One sentence that really rang true for me: ğ˜—ğ˜³ğ˜°ğ˜¥ğ˜¶ğ˜¤ğ˜µğ˜ªğ˜°ğ˜¯ğ˜ªğ˜»ğ˜ªğ˜¯ğ˜¨ ğ˜°1 ğ˜ªğ˜´ ğ˜®ğ˜¶ğ˜¤ğ˜© ğ˜©ğ˜¢ğ˜³ğ˜¥ğ˜¦ğ˜³ ğ˜µğ˜©ğ˜¢ğ˜¯ ğ˜¯ğ˜¢ğ˜ªğ˜­ğ˜ªğ˜¯ğ˜¨ ğ˜µğ˜©ğ˜¦ ğ˜¢ğ˜¤ğ˜¢ğ˜¥ğ˜¦ğ˜®ğ˜ªğ˜¤ ğ˜£ğ˜¦ğ˜¯ğ˜¤ğ˜©ğ˜®ğ˜¢ğ˜³ğ˜¬ğ˜´. ğŸ”— [https://lnkd.in/eHZWsPVb](https://lnkd.in/eHZWsPVb?trk=public_post_main-feed-card-text) ğŸ‘‰ ğ™ğ™šğ™™ğ™ªğ™˜ğ™ğ™£ğ™œ ğ™‡ğ™‡ğ™ˆ ğ™ğ™–ğ™¡ğ™¡ğ™ªğ™˜ğ™ğ™£ğ™–ğ™©ğ™ğ™¤ğ™£ğ™¨ ğ™ªğ™¨ğ™ğ™£ğ™œ ğ™ğ™šğ™©ğ™§ğ™ğ™šğ™«ğ™–ğ™¡ ğ™„ğ™£ğ™©ğ™šğ™§ğ™¡ğ™šğ™–ğ™«ğ™šğ™™ ğ™‚ğ™šğ™£ğ™šğ™§ğ™–ğ™©ğ™ğ™¤ğ™£ (ğ™ğ™„ğ™‚) by [Google](https://www.linkedin.com/company/google?trk=public_post_main-feed-card-text): I was not too familiar with this technique before DataGemma was released this week. And this one also fits into the broader topic of LLMs + Reasoning: Gemma 2 is being fine-tuned to recognise when it needs to replace a generated number with more accurate information from Data Commons. Think of it as the model double-checking its work against a trusted source. ğŸ”— [https://lnkd.in/egnHvBWv](https://lnkd.in/egnHvBWv?trk=public_post_main-feed-card-text) ğ™‡ğ™šğ™© ğ™¢ğ™š ğ™ ğ™£ğ™¤ğ™¬ ğ™¬ğ™ğ™ğ™˜ğ™ ğ™¤ğ™£ğ™š (ğ™ğ™› ğ™–ğ™£ğ™®) ğ™®ğ™¤ğ™ª ğ™›ğ™¤ğ™ªğ™£ğ™™ ğ™ğ™šğ™¡ğ™¥ğ™›ğ™ªğ™¡ ğŸ˜Š
    
    - 
        
        [](https://media.licdn.com/dms/image/v2/D4E22AQE1DcG9GLpQkg/feedshare-shrink_2048_1536/feedshare-shrink_2048_1536/0/1726376275363?e=2147483647&v=beta&t=Kq9Ur5W4Ljh1JbAOUV6M7wWOx2BMq01jn_MvbDp8Sao)
        
        No alternative text description for this image
        
    
    To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_what-a-week-huh-below-three-hand-picked-activity-7240970122423603200-YT4f&trk=public_post_main-feed-card_feed-cta-banner-cta)
    
- [Heiko Hotz](https://uk.linkedin.com/in/heikohotz/en?trk=public_post_main-feed-card_feed-actor-name)   2d Edited    [163](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F-%253F%253F-%253F%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F%253F-activity-7240613101526556673-skcp&trk=public_post_main-feed-card_social-actions-reactions)   [19 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F-%253F%253F-%253F%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F%253F-activity-7240613101526556673-skcp&trk=public_post_main-feed-card_social-actions-comments)   [Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F-%253F%253F-%253F%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F%253F-activity-7240613101526556673-skcp&trk=public_post_main-feed-card_like-cta)   [Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F-%253F%253F-%253F%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F%253F-activity-7240613101526556673-skcp&trk=public_post_main-feed-card_comment-cta)
    
    Generative AI Global Blackbelt @ Google â—† Founder of NLP London
    
    ğ—¢ğ—½ğ—²ğ—»ğ—”ğ—œ'ğ˜€ ğ—¼ğŸ­ ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—°ğ—¿ğ—²ğ—®ğ˜ğ—²ğ˜€ ğ—®ğ—» ğ—²ğ—»ğ˜ğ—¶ğ—¿ğ—²ğ—¹ğ˜† ğ—»ğ—²ğ˜„ ğ—Ÿğ—Ÿğ—  ğ˜ğ—¶ğ—²ğ—¿ In my experience there are currently four dimensions that organisations consider when deciding which LLMs to use in production. ğŸ‘‰ Quality ğŸ‘‰ Price ğŸ‘‰ Speed ğŸ‘‰ Context window size The order of importance for each of these dimensions depends on the use case. Up until this week the model landscape was roughly divided into two tiers (wrt price/speed/quality): 1ï¸âƒ£ Smaller models that are fast and inexpensive, but with slightly lower quality (GPT4o-mini, Gemini 1.5 Flash, Claude 3 Haiku, etc) 2ï¸âƒ£ Premium models with top quality but not as fast and a higher price tag (think GPT4o, Gemini 1.5 Pro, Claude 3.5 Sonnet, etc) In the two-tier landscape I have observed that small models are being used for high usage scenarios where speed and cost-efficiency are crucial, such as real-time chat applications, content moderation, or large-scale data processing tasks. These models are great in situations where a quick response is more valuable than perfect accuracy. Premium models, on the other hand, are being used by organisations when for tasks requiring high precision, complex reasoning, or nuanced understanding. They're often deployed into production in specialised fields like legal analysis, advanced research, or high-stakes decision-making processes. -------------------------------------------------------------- The arrival of o1 created an entire new tier, redefining the state of the art with its impressive quality, but which also comes at a price. The introduction of this new tier raises interesting questions about the future of LLM applications: while o1's impressive quality sets a new benchmark, its higher price point and potentially increased token usage (because of the reasoning chain) may limit its practical applications in many business scenarios. ğ™’ğ™ğ™–ğ™© ğ™–ğ™§ğ™š ğ™®ğ™¤ğ™ªğ™§ ğ™©ğ™ğ™¤ğ™ªğ™œğ™ğ™©ğ™¨ ğ™¤ğ™£ ğ™©ğ™ğ™ğ™¨ ğ™£ğ™šğ™¬ ğ™©ğ™ğ™šğ™§? ğ™ƒğ™¤ğ™¬ ğ™™ğ™¤ ğ™®ğ™¤ğ™ª ğ™¨ğ™šğ™š ğ™ğ™© ğ™›ğ™ğ™©ğ™©ğ™ğ™£ğ™œ ğ™ğ™£ğ™©ğ™¤ ğ™®ğ™¤ğ™ªğ™§ ğ™¤ğ™§ğ™œğ™–ğ™£ğ™ğ™¨ğ™–ğ™©ğ™ğ™¤ğ™£'ğ™¨ ğ˜¼ğ™„ ğ™¨ğ™©ğ™§ğ™–ğ™©ğ™šğ™œğ™®, ğ™ğ™› ğ™–ğ™© ğ™–ğ™¡ğ™¡? ğ˜¼ğ™§ğ™š ğ™©ğ™ğ™šğ™§ğ™š ğ™¨ğ™¥ğ™šğ™˜ğ™ğ™›ğ™ğ™˜ ğ™ªğ™¨ğ™š ğ™˜ğ™–ğ™¨ğ™šğ™¨ ğ™¬ğ™ğ™šğ™§ğ™š ğ™®ğ™¤ğ™ª ğ™—ğ™šğ™¡ğ™ğ™šğ™«ğ™š ğ™¤1'ğ™¨ ğ™˜ğ™–ğ™¥ğ™–ğ™—ğ™ğ™¡ğ™ğ™©ğ™ğ™šğ™¨ ğ™˜ğ™¤ğ™ªğ™¡ğ™™ ğ™¥ğ™§ğ™¤ğ™«ğ™ğ™™ğ™š ğ™¨ğ™ğ™œğ™£ğ™ğ™›ğ™ğ™˜ğ™–ğ™£ğ™© ğ™«ğ™–ğ™¡ğ™ªğ™š ğ™™ğ™šğ™¨ğ™¥ğ™ğ™©ğ™š ğ™ğ™©ğ™¨ ğ™ğ™ğ™œğ™ğ™šğ™§ ğ™˜ğ™¤ğ™¨ğ™©?
    
    - 
        
        [](https://media.licdn.com/dms/image/v2/D4E10AQH22m_8VeSNvA/image-shrink_800/image-shrink_800/0/1726296662240?e=2147483647&v=beta&t=Aw720_YH9Kk_TFjp5WGz1U3yVzT8KSZnRbDdwJJ-nW8)
        
        No alternative text description for this image
        
    
    To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F-%253F%253F-%253F%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F%253F-activity-7240613101526556673-skcp&trk=public_post_main-feed-card_feed-cta-banner-cta)
    
- [Heiko Hotz](https://uk.linkedin.com/in/heikohotz/en?trk=public_post_main-feed-card_feed-actor-name)   3d    [18](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_the-gen-z-and-gen-alpha-generations-are-apparently-activity-7240268756373901312-87s7&trk=public_post_main-feed-card_social-actions-reactions)   [9 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_the-gen-z-and-gen-alpha-generations-are-apparently-activity-7240268756373901312-87s7&trk=public_post_main-feed-card_social-actions-comments)   [Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_the-gen-z-and-gen-alpha-generations-are-apparently-activity-7240268756373901312-87s7&trk=public_post_main-feed-card_like-cta)   [Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_the-gen-z-and-gen-alpha-generations-are-apparently-activity-7240268756373901312-87s7&trk=public_post_main-feed-card_comment-cta)
    
    Generative AI Global Blackbelt @ Google â—† Founder of NLP London
    
    The Gen Z and Gen Alpha generations are apparently not very intelligent. I mean, how else could we possibly explain that they have not produced a single Nobel Prize winner??? ğŸ¤” [https://lnkd.in/eXmazpAc](https://lnkd.in/eXmazpAc?trk=public_post_main-feed-card-text)
    
    - 
        
        [](https://media.licdn.com/dms/image/v2/D4E22AQHKVBEbTIcwRA/feedshare-shrink_800/feedshare-shrink_800/0/1726214587993?e=2147483647&v=beta&t=Q9vlddNBisCNMicD9ACnEoyLYSz7TFihDfpHdez6zAU)
        
        No alternative text description for this image
        
    
    To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_the-gen-z-and-gen-alpha-generations-are-apparently-activity-7240268756373901312-87s7&trk=public_post_main-feed-card_feed-cta-banner-cta)
    
- [Heiko Hotz](https://uk.linkedin.com/in/heikohotz/en?trk=public_post_main-feed-card_feed-actor-name)   3d    [29](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F-%253F%253F%253F-activity-7240007899219963904-BnvR&trk=public_post_main-feed-card_social-actions-reactions)   [5 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F-%253F%253F%253F-activity-7240007899219963904-BnvR&trk=public_post_main-feed-card_social-actions-comments)   [Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F-%253F%253F%253F-activity-7240007899219963904-BnvR&trk=public_post_main-feed-card_like-cta)   [Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F-%253F%253F%253F-activity-7240007899219963904-BnvR&trk=public_post_main-feed-card_comment-cta)
    
    Generative AI Global Blackbelt @ Google â—† Founder of NLP London
    
    ğ——ğ—®ğ˜ğ—® ğ—–ğ—¼ğ—ºğ—ºğ—¼ğ—»ğ˜€ + ğ—¥ğ—œğ—š + ğ—¥ğ—”ğ—š = ğ——ğ—®ğ˜ğ—®ğ—šğ—²ğ—ºğ—ºğ—® ğŸ’ª [Google](https://www.linkedin.com/company/google?trk=public_post_main-feed-card-text) just released two DataGemma models which use real world data to tackle AI hallucinations. Both models use Data Commons, a publicly available knowledge graph containing over 240 ğ™—ğ™ğ™¡ğ™¡ğ™ğ™¤ğ™£ ğ™§ğ™ğ™˜ğ™ ğ™™ğ™–ğ™©ğ™– ğ™¥ğ™¤ğ™ğ™£ğ™©ğ™¨ from trusted organisations like the United Nations (UN), Center for Disease Control and Prevention (CDC) and global census bureaus. DataGemma comes in two flavours with two distinct approaches: 1ï¸âƒ£ ğ™ğ™„ğ™‚ (ğ™ğ™šğ™©ğ™§ğ™ğ™šğ™«ğ™–ğ™¡-ğ™„ğ™£ğ™©ğ™šğ™§ğ™¡ğ™šğ™–ğ™«ğ™šğ™™ ğ™‚ğ™šğ™£ğ™šğ™§ğ™–ğ™©ğ™ğ™¤ğ™£): This model has been fine-tuned to generate natural language queries for Data Commons alongside its regular text output, enabling real-time fact-checking of statistical claims by retrieving verified data. An example for a RIG query and response would be: ğ˜œğ˜´ğ˜¦ğ˜³: ğ˜ğ˜©ğ˜¢ğ˜µ'ğ˜´ ğ˜µğ˜©ğ˜¦ ğ˜¢ğ˜·ğ˜¦ğ˜³ğ˜¢ğ˜¨ğ˜¦ ğ˜´ğ˜ªğ˜»ğ˜¦ ğ˜°ğ˜§ ğ˜¢ ğ˜œğ˜š ğ˜©ğ˜°ğ˜¶ğ˜´ğ˜¦ğ˜©ğ˜°ğ˜­ğ˜¥? ğ˜‹ğ˜¢ğ˜µğ˜¢ğ˜ğ˜¦ğ˜®ğ˜®ğ˜¢: ğ˜›ğ˜©ğ˜¦ ğ˜¢ğ˜·ğ˜¦ğ˜³ğ˜¢ğ˜¨ğ˜¦ ğ˜´ğ˜ªğ˜»ğ˜¦ ğ˜°ğ˜§ ğ˜¢ ğ˜œğ˜š ğ˜©ğ˜°ğ˜¶ğ˜´ğ˜¦ğ˜©ğ˜°ğ˜­ğ˜¥ ğ˜ªğ˜´ [ğ˜‹ğ˜Š("ğ˜¸ğ˜©ğ˜¢ğ˜µ ğ˜¸ğ˜¢ğ˜´ ğ˜µğ˜©ğ˜¦ ğ˜¢ğ˜·ğ˜¦ğ˜³ğ˜¢ğ˜¨ğ˜¦ ğ˜©ğ˜°ğ˜¶ğ˜´ğ˜¦ğ˜©ğ˜°ğ˜­ğ˜¥ ğ˜´ğ˜ªğ˜»ğ˜¦ ğ˜ªğ˜¯ ğ˜µğ˜©ğ˜¦ ğ˜œğ˜¯ğ˜ªğ˜µğ˜¦ğ˜¥ ğ˜šğ˜µğ˜¢ğ˜µğ˜¦ğ˜´ ğ˜ªğ˜¯ 2021?") --> "2.53"] ğ˜±ğ˜¦ğ˜°ğ˜±ğ˜­ğ˜¦. As we can see, the RIG model produces a response that includes both the LLM-generated text and the Data Commons queries embedded within it. And then there is a post-processing step that resolves the Data Commons queries and replaces them with actual data, before sending the final response to the user. 2ï¸âƒ£ ğ™ğ˜¼ğ™‚ (ğ™ğ™šğ™©ğ™§ğ™ğ™šğ™«ğ™–ğ™¡-ğ˜¼ğ™ªğ™œğ™¢ğ™šğ™£ğ™©ğ™šğ™™ ğ™‚ğ™šğ™£ğ™šğ™§ğ™–ğ™©ğ™ğ™¤ğ™£): This model's role is limited to generating relevant questions for Data Commons based on the user's query. The response is then passed on to Gemini 1.5 Pro, leveraging its long context window to produce the final response. So this model is only a part of a larger LLM application. ğ—¥Ì²ğ—²Ì²ğ˜€Ì²ğ˜‚Ì²ğ—¹Ì²ğ˜Ì²ğ˜€Ì² ğ™ğ™„ğ™‚: ğŸ‘‰ Improved factual accuracy from 5-17% to about 58% ğŸ‘‰ User preference: 62% (7B model) to 76% (27B model) over baseline ğ™ğ˜¼ğ™‚: ğŸ‘‰ Statistical claims accuracy: 98.6-98.9% ğŸ‘‰ Coverage: 24-29% of queries had statistical responses ğŸ‘‰ User preference: 92-100% when Data Commons stats were used Both approaches showed significant improvements over baseline models in terms of factual accuracy and user preference, with RAG generally performing better when data was available. Blog post, technical paper, and model links below in the comments ğŸ¤—
    
    - 
        
        [](https://media.licdn.com/dms/image/v2/D4E22AQGJpeZUEE52Yg/feedshare-shrink_800/feedshare-shrink_800/0/1726152395191?e=2147483647&v=beta&t=nvc2qkAbCszZ1LHhL_9H34YXFmHPk2884k6FW2mvHDg)
        
        No alternative text description for this image
        
    
    To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F-%253F%253F%253F-activity-7240007899219963904-BnvR&trk=public_post_main-feed-card_feed-cta-banner-cta)
    
- [Heiko Hotz](https://uk.linkedin.com/in/heikohotz/en?trk=public_post_main-feed-card_feed-actor-name)   4d    [18](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F-%253F%253F-%253F-activity-7239887462699868161-y917&trk=public_post_main-feed-card_social-actions-reactions)   [4 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F-%253F%253F-%253F-activity-7239887462699868161-y917&trk=public_post_main-feed-card_social-actions-comments)   [Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F-%253F%253F-%253F-activity-7239887462699868161-y917&trk=public_post_main-feed-card_like-cta)   [Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F-%253F%253F-%253F-activity-7239887462699868161-y917&trk=public_post_main-feed-card_comment-cta)
    
    Generative AI Global Blackbelt @ Google â—† Founder of NLP London
    
    ğ—¡ğ—¼ğ˜ğ—²ğ—¯ğ—¼ğ—¼ğ—¸ğ—Ÿğ—  ğ—¹ğ—²ğ—®ğ—¿ğ—»ğ˜€ ğ˜ğ—¼ ğ˜€ğ—½ğ—²ğ—®ğ—¸ ğŸ¤ Just a few days ago I posted about [Google](https://www.linkedin.com/company/google?trk=public_post_main-feed-card-text)'s Illuminate that will turn PDFs (for example research papers) into podcasts. Not NotbookLM has followed suit and introduced Audio Overview. With this new feature, users can now listen to AI-generated discussions about their sources. These discussions summarise the material, make connections between topics, and can be downloaded for on-the-go listening. I personally process and retain information more effectively when it's presented in spoken form. So while the dialogue is not perfect and not a replacement for real podcasts (for me anyway), I find it a great learning tool ğŸ¤— The features is now generally available, so go and try it out yourself.
    
    - 
        
        [](https://media.licdn.com/dms/image/v2/D4E22AQEtFT6vVO5pOw/feedshare-shrink_800/feedshare-shrink_800/0/1726123680156?e=2147483647&v=beta&t=njxk7S2tU6ZgaIQVT2xSf75bVTS31gdaYDaJGqNlk9A)
        
        No alternative text description for this image
        
    
    To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F-%253F%253F-%253F-activity-7239887462699868161-y917&trk=public_post_main-feed-card_feed-cta-banner-cta)
    
- [Heiko Hotz](https://uk.linkedin.com/in/heikohotz/en?trk=public_post_main-feed-card_feed-actor-name)   5d    [19](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F-%253F%253F%253F-%253F%253F%253F-use-control-generated-activity-7239508597758439424-JUCG&trk=public_post_main-feed-card_social-actions-reactions)   [7 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F-%253F%253F%253F-%253F%253F%253F-use-control-generated-activity-7239508597758439424-JUCG&trk=public_post_main-feed-card_social-actions-comments)   [Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F-%253F%253F%253F-%253F%253F%253F-use-control-generated-activity-7239508597758439424-JUCG&trk=public_post_main-feed-card_like-cta)   [Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F-%253F%253F%253F-%253F%253F%253F-use-control-generated-activity-7239508597758439424-JUCG&trk=public_post_main-feed-card_comment-cta)
    
    Generative AI Global Blackbelt @ Google â—† Founder of NLP London
    
    ğ—£ğ—¿ğ—¼ ğ—Ÿğ—Ÿğ—  ğ—§ğ—¶ğ—½ Use control-generated output â€” because life with LLMs is already hard enough ğŸ˜ âš ï¸ ğ™ğ™§ğ™ğ™œğ™œğ™šğ™§ ğ™¬ğ™–ğ™§ğ™£ğ™ğ™£ğ™œ: ğ™ğ™¤ğ™¢ğ™š ğ™¤ğ™› ğ™©ğ™ğ™š ğ™§ğ™šğ™¨ğ™¥ğ™¤ğ™£ğ™¨ğ™šğ™¨ ğ™ğ™£ ğ™©ğ™ğ™š ğ™ğ™¢ğ™–ğ™œğ™š ğ™—ğ™šğ™¡ğ™¤ğ™¬ ğ™–ğ™§ğ™š ğ™™ğ™šğ™šğ™¢ğ™šğ™™ ğ™ğ™ğ™œğ™ğ™¡ğ™® ğ™˜ğ™¤ğ™£ğ™©ğ™§ğ™¤ğ™«ğ™šğ™§ğ™¨ğ™ğ™–ğ™¡ ğ™–ğ™£ğ™™ ğ™¢ğ™ğ™œğ™ğ™© ğ™˜ğ™–ğ™ªğ™¨ğ™š ğ™®ğ™¤ğ™ªğ™§ ğ™—ğ™¡ğ™¤ğ™¤ğ™™ ğ™¥ğ™§ğ™šğ™¨ğ™¨ğ™ªğ™§ğ™š ğ™©ğ™¤ ğ™§ğ™ğ™¨ğ™š ğŸ˜œ âš ï¸
    
    - 
        
        [](https://media.licdn.com/dms/image/v2/D4E22AQHtjC7ko0rqHg/feedshare-shrink_800/feedshare-shrink_800/0/1726033352503?e=2147483647&v=beta&t=An9B6y5F4Fl-bc2rGz08dwO09tY8CvC03QncE6cbPU8)
        
        No alternative text description for this image
        
    
    To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F-%253F%253F%253F-%253F%253F%253F-use-control-generated-activity-7239508597758439424-JUCG&trk=public_post_main-feed-card_feed-cta-banner-cta)
    
- [Heiko Hotz](https://uk.linkedin.com/in/heikohotz/en?trk=public_post_main-feed-card_feed-actor-name)   6d    [43](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F%253F-activity-7239153435013500928-G406&trk=public_post_main-feed-card_social-actions-reactions)   [4 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F%253F-activity-7239153435013500928-G406&trk=public_post_main-feed-card_social-actions-comments)   [Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F%253F-activity-7239153435013500928-G406&trk=public_post_main-feed-card_like-cta)   [Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F%253F-activity-7239153435013500928-G406&trk=public_post_main-feed-card_comment-cta)
    
    Generative AI Global Blackbelt @ Google â—† Founder of NLP London
    
    ğ—§ğ—¿ğ—®ğ—»ğ˜€ğ—³ğ—¼ğ—¿ğ—º ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—°ğ—¼ğ—»ğ˜ğ—²ğ—»ğ˜ ğŸ“• ğ—¶ğ—»ğ˜ğ—¼ ğ—® ğ—½ğ—¼ğ—±ğ—°ğ—®ğ˜€ğ˜ ğŸ¤ Last night I tried [Google](https://www.linkedin.com/company/google?trk=public_post_main-feed-card-text)'s ğ™„ğ™¡ğ™¡ğ™ªğ™¢ğ™ğ™£ğ™–ğ™©ğ™š for the first time, and I think my jaw dropped on the floor for 7 minutes straight ğŸ¤¯ ğ™„ğ™¡ğ™¡ğ™ªğ™¢ğ™ğ™£ğ™–ğ™©ğ™š lets you convert research papers (and probably other PDFs) into podcasts - you just paste in the URL of a PDF, press a button, and voilÃ : your podcast is ready. For me as a regular podcast listener this is huge. Admittedly, the dialogue is a bit clunky - I had the model create a podcast about the AI Scientist paper from a few weeks ago and I believe the "guest" on the podcast answered every single question with "The AI Scientist framework ..." ğŸ˜… But, as always, this is the worst this technology will ever be going forward, and that is exciting. Especially when we get more influence over the how the podcast is created (length/tone/details/etc) and maybe even interact with it ("A listener has a question ...") ğŸ¤— ğ™„ğ™¡ğ™¡ğ™ªğ™¢ğ™ğ™£ğ™–ğ™©ğ™š has been released for public preview, link in the comments below.
    
    To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F%253F-%253F%253F%253F%253F%253F%253F%253F-activity-7239153435013500928-G406&trk=public_post_main-feed-card_feed-cta-banner-cta)
    
- [Heiko Hotz](https://uk.linkedin.com/in/heikohotz/en?trk=public_post_main-feed-card_feed-actor-name)   1w Edited    [70](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F-%253F%253F%253F%253F%253F%253F-activity-7238796815486111746-d6K-&trk=public_post_main-feed-card_social-actions-reactions)   [11 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F-%253F%253F%253F%253F%253F%253F-activity-7238796815486111746-d6K-&trk=public_post_main-feed-card_social-actions-comments)   [Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F-%253F%253F%253F%253F%253F%253F-activity-7238796815486111746-d6K-&trk=public_post_main-feed-card_like-cta)   [Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F-%253F%253F%253F%253F%253F%253F-activity-7238796815486111746-d6K-&trk=public_post_main-feed-card_comment-cta)
    
    Generative AI Global Blackbelt @ Google â—† Founder of NLP London
    
    ğ—¥ğ—²ğ—³ğ—¹ğ—²ğ—°ğ˜ğ—¶ğ—¼ğ—» ğŸ³ğŸ¬ğ—• ğ—¨ğ—½ğ—±ğ—®ğ˜ğ—² ğŸ‘‰ Over the weekend and new model called Reflection 70B made a splash in the AI community, claiming impressive results on several benchmarks and beating GPT4o and Claude 3.5. ğŸ‘‰ Independent reviewers like [Artificial Analysis](https://www.linkedin.com/company/artificial-analysis?trk=public_post_main-feed-card-text) were not able to reproduce the results ğŸ‘‰ The creators of the model identified an issue with the weights of the model that they published on [Hugging Face](https://www.linkedin.com/company/huggingface?trk=public_post_main-feed-card-text) ğŸ‘‰ On Sunday, the creators re-trained and re-tested the model and they think they fixed the issue ğŸ‘‰ They made the new version available on OpenRouter (i.e. behind an API with no access to the weights) ğŸ‘‰ When testing this new version, users hints that it was a wrapper around Claude Sonnet 3.5: [https://lnkd.in/eadNf2R7](https://lnkd.in/eadNf2R7?trk=public_post_main-feed-card-text) ğŸ‘‰ [Artificial Analysis](https://www.linkedin.com/company/artificial-analysis?trk=public_post_main-feed-card-text) was given access to a private API for re-evaluation and was able to confirm the new results ğŸ‘‰ After that, new weights of the model were published on [https://lnkd.in/ejC2ntPi](https://lnkd.in/ejC2ntPi?trk=public_post_main-feed-card-text) ğŸ‘‰ [Artificial Analysis](https://www.linkedin.com/company/artificial-analysis?trk=public_post_main-feed-card-text) was NOT able to reproduce the claimed resulst with these new model weights and has questions: [https://lnkd.in/eH9jPvQp](https://lnkd.in/eH9jPvQp?trk=public_post_main-feed-card-text) --------------------------------------- Something clearly has gone wrong here, but here is what I would say: (1) I still believe that the technique of reflection in an LLM application can improve performance. (2) If I had been the one who created this new model and would have seen the crazy buzz and interest around it there would have been a 100% chance I would have been overwhelmed as well and would have made some unforced errors. Let's wait and see - and let's remember to be kind! The good thing is the technique is not new and well known (finetuning with synthetic dataset) and also not too expensive, so if push comes to shove others can try to build a similar model in their own time without any distracting buzz around it.
    
    - 
        
        [](https://media.licdn.com/dms/image/v2/D4E22AQGJMbHavQcAew/feedshare-shrink_800/feedshare-shrink_800/0/1725863649923?e=2147483647&v=beta&t=LMIN04DMEsPbUPL9nK2yiWypVi1pOpR3YsibMYzljO4)
        
        No alternative text description for this image
        
    
    To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F%253F-%253F%253F%253F%253F%253F%253F-activity-7238796815486111746-d6K-&trk=public_post_main-feed-card_feed-cta-banner-cta)
    
- [Heiko Hotz](https://uk.linkedin.com/in/heikohotz/en?trk=public_post_main-feed-card_feed-actor-name)   1w Edited   [48](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F-%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-activity-7238452200165838848-Kw_f&trk=public_post_main-feed-card_social-actions-reactions)   [16 Comments](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F-%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-activity-7238452200165838848-Kw_f&trk=public_post_main-feed-card_social-actions-comments)   [Like](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F-%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-activity-7238452200165838848-Kw_f&trk=public_post_main-feed-card_like-cta)   [Comment](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F-%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-activity-7238452200165838848-Kw_f&trk=public_post_main-feed-card_comment-cta)
    
    Generative AI Global Blackbelt @ Google â—† Founder of NLP London
    
    ğ—¥ğ—²ğ—³ğ—¹ğ—²ğ—°ğ˜ğ—¶ğ—»ğ—´ ğ—¼ğ—» ğ™ğ™šğ™›ğ™¡ğ™šğ™˜ğ™©ğ™ğ™¤ğ™£ 70ğ˜½ !!! ğ—œğ— ğ—£ğ—¢ğ—¥ğ—§ğ—”ğ—¡ğ—§ ğ—¨ğ—£ğ——ğ—”ğ—§ğ—˜ ğŸ‘‰ (ğŸ´ ğ—¦ğ—²ğ—½, ğŸ­ğ—½ğ—º ğ—•ğ—¦ğ—§): ğ—”ğ—½ğ—½ğ—®ğ—¿ğ—²ğ—»ğ˜ğ—¹ğ˜† ğ˜ğ—µğ—²ğ—¿ğ—² ğ˜„ğ—®ğ˜€ ğ—®ğ—» ğ—¶ğ˜€ğ˜€ğ˜‚ğ—² ğ˜„ğ—¶ğ˜ğ—µ ğ˜ğ—µğ—² ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ˜„ğ—µğ—²ğ—» ğ˜ğ—µğ—² ğ—®ğ˜‚ğ˜ğ—µğ—¼ğ—¿ğ˜€ ğ—³ğ—¶ğ—¿ğ˜€ğ˜ ğ˜‚ğ—½ğ—¹ğ—¼ğ—®ğ—±ğ—²ğ—± ğ—¶ğ˜. They are working on it, you can follow Matt on Twitter for updates (e.g. [https://lnkd.in/e2swBkuB](https://lnkd.in/e2swBkuB?trk=public_post_main-feed-card-text)) ğŸ‘‰ (ğŸµ ğ—¦ğ—²ğ—½, ğŸ³ğ—®ğ—º ğ—•ğ—¦ğ—§): ğ—§ğ—µğ—² ğ—®ğ˜‚ğ˜ğ—µğ—¼ğ—¿ğ˜€ ğ˜‚ğ—½ğ—¹ğ—¼ğ—®ğ—±ğ—²ğ—± ğ—»ğ—²ğ˜„ ğ˜„ğ—²ğ—¶ğ—´ğ—µğ˜ğ˜€ ğ—®ğ—»ğ—± ğ˜ğ—µğ—²ğ˜† ğ—¯ğ—²ğ—¹ğ—¶ğ—²ğ˜ƒğ—² ğ˜ğ—µğ—² ğ—¶ğ˜€ğ˜€ğ˜‚ğ—² ğ—¶ğ˜€ ğ—³ğ—¶ğ˜…ğ—²ğ—±. [Artificial Analysis](https://www.linkedin.com/company/artificial-analysis?trk=public_post_main-feed-card-text) cannot confirm and has questions: [https://lnkd.in/eH9jPvQp](https://lnkd.in/eH9jPvQp?trk=public_post_main-feed-card-text) !!! A few months ago I was discussing with [James Huckle](https://uk.linkedin.com/in/james-huckle?trk=public_post_main-feed-card-text) on how to improve the performance of LLMs where they make seemingly â€œeasyâ€ mistakes ("easy" from a human point of view). I suggested a mechanism where the model can ğ—¿ğ—²ğ—³ğ—¹ğ—²ğ—°ğ˜ on its immediate response in order to identify any mistakes it might have made and ğ—´ğ—¶ğ˜ƒğ—² ğ˜ğ—µğ—² ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—® ğ—°ğ—µğ—®ğ—»ğ—°ğ—² ğ˜ğ—¼ ğ—¿ğ—²ğ—°ğ—¼ğ˜ƒğ—²ğ—¿. [Matt Shumer](https://www.linkedin.com/in/mattshumer?trk=public_post_main-feed-card-text) & [Sahil Chaudhary](https://nl.linkedin.com/in/sahil-chaudhary-981428132?trk=public_post_main-feed-card-text) had a similar idea and simplified it even further: They fine-tuned a Llama-3.1 70B model to have this ğ—¿ğ—²ğ—³ğ—¹ğ—²ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ—ºğ—¼ğ—±ğ—² ğ—¯ğ˜‚ğ—¶ğ—¹ğ˜ ğ—¶ğ—». So, rather than having to make two LLM calls this model does it all in one. This model is called ğ™ğ™šğ™›ğ™¡ğ™šğ™˜ğ™©ğ™ğ™¤ğ™£ 70ğ˜½ and it seems a 405B version is coming soon. And it seems that the idea of making LLMs reflect is rather successful - Reflection 70B is crushing several SOTA models on a range of benchmarks. Now, it is worth noting that this is not a great model to â€œjust have a chat withâ€. In my experience it will always print its reflections before responding, so a simple â€œHelloâ€ exchange can be quite â€œinterestingâ€ ğŸ˜‰ ğ˜‰ğ˜º ğ˜µğ˜©ğ˜¦ ğ˜¸ğ˜¢ğ˜º, ğ˜µğ˜©ğ˜¦ ğ˜®ğ˜°ğ˜¥ğ˜¦ğ˜­ (ğ˜²ğ˜¶ğ˜¢ğ˜¯ğ˜µğ˜ªğ˜»ğ˜¦ğ˜¥ ğ˜·ğ˜¦ğ˜³ğ˜´ğ˜ªğ˜°ğ˜¯) ğ˜³ğ˜¶ğ˜¯ğ˜´ ğ˜¢ğ˜µ ğ˜¢ ğ˜¥ğ˜¦ğ˜¤ğ˜¦ğ˜¯ğ˜µ ğ˜´ğ˜±ğ˜¦ğ˜¦ğ˜¥ ğ˜­ğ˜°ğ˜¤ğ˜¢ğ˜­ğ˜­ğ˜º ğ˜°ğ˜¯ ğ˜¢ ğ˜”ğ˜¢ğ˜¤ğ˜‰ğ˜°ğ˜°ğ˜¬ ğ˜—ğ˜³ğ˜° ğ˜”1 (64ğ˜ğ˜‰ ğ˜™ğ˜ˆğ˜”). ğ˜ğ˜§ ğ˜ºğ˜°ğ˜¶ ğ˜©ğ˜¢ğ˜·ğ˜¦ ğ˜¢ ğ˜´ğ˜ªğ˜®ğ˜ªğ˜­ğ˜¢ğ˜³ğ˜­ğ˜º ğ˜´ğ˜±ğ˜¦ğ˜¤â€™ğ˜¥ ğ˜®ğ˜¢ğ˜¤ğ˜©ğ˜ªğ˜¯ğ˜¦ ğ˜¢ğ˜¯ğ˜¥ ğ˜¸ğ˜°ğ˜¶ğ˜­ğ˜¥ ğ˜­ğ˜ªğ˜¬ğ˜¦ ğ˜µğ˜° ğ˜¬ğ˜¯ğ˜°ğ˜¸ ğ˜®ğ˜°ğ˜³ğ˜¦ ğ˜©ğ˜°ğ˜¸ ğ˜µğ˜° ğ˜´ğ˜¦ğ˜µ ğ˜ªğ˜µ ğ˜¶ğ˜±, ğ˜­ğ˜¦ğ˜µ ğ˜®ğ˜¦ ğ˜¬ğ˜¯ğ˜°ğ˜¸ - ğ˜ ğ˜®ğ˜ªğ˜¨ğ˜©ğ˜µ ğ˜®ğ˜¢ğ˜¬ğ˜¦ ğ˜¢ ğ˜²ğ˜¶ğ˜ªğ˜¤ğ˜¬ ğ˜µğ˜¶ğ˜µğ˜°ğ˜³ğ˜ªğ˜¢ğ˜­ ğ˜°ğ˜¯ ğ˜µğ˜©ğ˜¢ğ˜µ.
    
    To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fheikohotz_%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-%253F%253F-%253F%253F%253F%253F%253F%253F%253F%253F%253F%253F-activity-7238452200165838848-Kw_f&trk=public_post_main-feed-card_feed-cta-banner-cta)