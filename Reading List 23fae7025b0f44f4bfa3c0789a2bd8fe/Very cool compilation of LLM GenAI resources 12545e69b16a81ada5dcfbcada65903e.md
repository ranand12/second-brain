# Very cool compilation of LLM GenAI resources

Column: https://www.linkedin.com/feed/update/urn:li:activity:7253724104619438080
Processed: Yes
created on: October 20, 2024 7:49 PM

This is a quite big deal for [#FastInference](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Ffastinference&trk=public_post_main-feed-card-text) - welcome [#LayerSkip](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Flayerskip&trk=public_post_main-feed-card-text)! ğŸ‘‰ LayerSkip (as the name suggests) is about "early exit" inference and "self-speculative" decoding. What does it mean? It's to speed up the inference process via some smart tricks (both during training and inference): 1ï¸âƒ£ During training, layer dropout is applied, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit 2ï¸âƒ£ During inference, this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model ğŸ‘‰ A novel self-speculative decoding solution is used, which consists of 2 key steps: 4ï¸âƒ£ Self-Drafting, using the early exit to draft tokens from the same model 5ï¸âƒ£ Self-Verification, using the remaining layers to validate the prediction 6ï¸âƒ£ To enable re-use in both these, a novel Cache Reuse technique is used to unify the KV cache and storing the exit query â“And the results? ğŸ‘‰ Proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages. ğŸ‘‰ With experiments on different [#Llama](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Ffeed%2Fhashtag%2Fllama&trk=public_post_main-feed-card-text) model sizes on different types of training (pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task) it was demonstrated, that speedup on summarization for CNN/DM documents is up to 2.16x, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task. ğŸ¯ Code: [https://lnkd.in/dqtjzfph](https://lnkd.in/dqtjzfph?trk=public_post_main-feed-card-text) ğŸ¯ Open weights for selected models: [https://lnkd.in/dc-maxwc](https://lnkd.in/dc-maxwc?trk=public_post_main-feed-card-text) Consists of: llama2-7B, llama2-13B, codellama-7B, codellama-34B, llama3-8B and llama3.2-1B ğŸ“© FAIR Noncommercial Research License, approval required ğŸ¯ Paper: [https://lnkd.in/dqRBNfwW](https://lnkd.in/dqRBNfwW?trk=public_post_main-feed-card-text) Note: the paper is not new, but now the complete stack was presented.

To view or add a comment, [sign in](https://www.linkedin.com/signup/cold-join?session_redirect=https%3A%2F%2Fwww.linkedin.com%2Fposts%2Fpetr-kazar_fastinference-layerskip-llama-activity-7253114295691972609-WzZE&trk=public_post_main-feed-card_feed-cta-banner-cta)